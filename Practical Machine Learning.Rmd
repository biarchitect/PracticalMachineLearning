---
title: "Practical Machine Learning Project"
author: "Todd H. Robinson"
date: "Friday, July 25, 2014"
output: html_document
---

##Executive Summary
The goal of this project is to predict the manner in which a group of people did certain exercises.
We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants as they exercise.
Random Forest function was used to build a prediction model.
The data was downloaded from here;
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.

This data will be cleansed and partitioned into training and validation sets in order to build an efficient and accurate prediction model.
The model will then be tested against a test dataset downloaded from here;

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The model produced was efficient in that it took a little over 3 minutes to build.
The training set was built from 50% of the training data.
It contains 56 variables of interest and was 100% accurate on both the validation and test datasets.
The Out of sample error was very small. THe model built resulted in an accuracy of 0.9925 within a confidence interval of (0.9905, 0.9941) with a p-value of 0. 
The model was 100% accurate againest the 20 test cases.

##Methodology
###Step 1 - Download the data
```{r echo=TRUE}
filename<-"./Data/pml-training.csv"
data<-read.csv(filename,header=T,stringsAsFactors=F)
```
###Step 2 - Data cleansing
Remove all variables that do not contain a high percentage of data. Actually all variables that contain at least one NA observation.
``` {r echo=TRUE}
temp<-data
#The first four variables are row counts, timestamps, who did the exercise and the window.
#Remove them as they add nothing to the predicive goal of the model.
temp<-temp[,-1:-7]
#Make any empty cells NA
temp[temp==""]<-NA
#Make all bad Math cells NA
temp[temp=="#DIV/0!"]<-NA

#Get rid of all variables that are ALL NA
temp<-temp[,!apply(temp,2,function(x) all(is.na(x)))]
#Remove all variables with less than NApct
NApct<-1
m<-dim(temp)[1];x<-1;y<-dim(temp)[2]
while (x < y){if (sum(!is.na(temp[,x]))/m < NApct) {temp[,x]<-NULL;y<-dim(temp)[2]} else{ x<-x+1}}

temp<-apply(temp[,c(-53)],2,as.numeric)
temp<-as.data.frame(temp)
#Put back and make outcome varialbe a factor
temp$classe<-as.factor(data[,160])

```

###Step 3 - Partition the cleansed dataset
``` {r echo=TRUE}
library(caret)
set.seed(658)
#Partition 50/50
part<-0.50
training<-createDataPartition(temp$classe, p = .50)[[1]]

train<-temp[training,]
vdate<-temp[-training,]

```
###Step 4  - Build the model

``` {r echo=TRUE}
library(randomForest)
modelFit<-randomForest(classe~.,data=train)
#Validate 
confusionMatrix(vdate$classe,predict(modelFit,newdata=vdate[,-53]))
```


### Generate out of sample error
###Step 5 - Test model
##Conclusion









```{r}
summary(cars)
```

You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
