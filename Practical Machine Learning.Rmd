---
title: "Practical Machine Learning Project"
author: "Todd H. Robinson"
date: "Friday, July 25, 2014"
output: html_document
---

##Executive Summary
The goal of this project is to predict the manner in which a group of people did certain exercises.
We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants as they exercise.
The Random Forest algorithm found in the randomForest library was used to build a prediction model. The model was evaluated using the customMatrix and predict functions from the caret library.
The data was downloaded from here;

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.

This data will be cleansed and partitioned into training and validation sets in order to build an efficient and accurate prediction model.
The model will then be tested against a test dataset downloaded from here;

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

##Conclusion

The training set was built from 50% of the training data with the other 50% for validation.  This Random Forest model was used for the test submission part of the project.
I then decided to introduce as much bias as possible and so built a model with 0.05% of the data. This reduced the accuracy of the model to 89%. Still pretty good considering the model was trained on less than 1000 observations. This led me to believe the model was good enough.
The model contains 52 variables of interest and was 99.25% accurate on both the validation and 100% accurate on the limited test dataset after training on 50% of the training data. The out of sample error was very small; 34/9810 = 0.0035.
The model produced was efficient in that it took a less than 3 minutes to build.
The model was extremely accurate. The accuracy was 0.9925 within a confidence interval of (0.9905, 0.9941) with a p-value of approximately 0. 
The model was 100% accurate againest the 20 test cases.
I fully expected to have to employ a variable reduction strategy specifically PCA but the model built quickly and accurately and therefore a further reduction in variables was not necessary. I also expected that my variable pruning was way too agressive but the model results were again such that it was not necessary. 

##Methodology
###Step 1 - Download the data
```{r echo=TRUE}
filename<-"./Data/pml-training.csv"
data<-read.csv(filename,header=T,stringsAsFactors=F)
```
###Step 2 - Data cleansing
Remove all variables that do not contain a high percentage of data. Actually all variables that contain at least one NA observation.
 
``` {r echo=TRUE}
cleandata<-function(temp){
  
  #The first four variables are row counts, timestamps, who did the exercise and the window.
  #Remove them as they add nothing to the predicive goal of the model.
  temp<-temp[,-1:-7]
  #Make any empty cells NA
  temp[temp==""]<-NA
  #Make all bad Math cells NA
  temp[temp=="#DIV/0!"]<-NA
  
  #Get rid of all variables that are ALL NA
  temp<-temp[,!apply(temp,2,function(x) all(is.na(x)))]
  #Remove all variables with less than NApct of real data.
  NApct<-1
  m<-dim(temp)[1];x<-1;y<-dim(temp)[2]
  while (x < y){if (sum(!is.na(temp[,x]))/m < NApct) {temp[,x]<-NULL;y<-dim(temp)[2]} else{ x<-x+1}}
  
  temp<-apply(temp[,c(-53)],2,as.numeric)
  temp<-as.data.frame(temp)
  return(temp)
}
temp<-cleandata(data)
temp$classe<-as.factor(data[,160])

```

###Step 3 - Partition the cleansed dataset


``` {r echo=TRUE}
library(caret)
set.seed(658)
#Partition 50/50
part<-0.50
training<-createDataPartition(temp$classe, p = part)[[1]]

train<-temp[training,]
vdate<-temp[-training,]

```
###Step 4  - Build the model

``` {r echo=TRUE}
library(randomForest)
modelFit<-randomForest(classe~.,data=train)
#Validate 
confusionMatrix(vdate$classe,predict(modelFit,newdata=vdate[,-53]))
```

###Step 5 - Test model

``` {r echo=TRUE}
filename<-"./Data/pml-testing.csv"
#Get Data
dataT<-read.csv(filename,stringsAsFactors=F)
#Clean Data
test<-cleandata(dataT)
#Add Outcome Variable to Test set
test$problem_id<-as.factor(dataT[,160])
#Create a prdiction
x<-predict(modelFit,newdata=test)
#Test those predictions againest the 20 predictions submitted to Course as correct.
good<-as.factor(c("B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B"))
confusionMatrix(good,x)
```
###Citations
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 


